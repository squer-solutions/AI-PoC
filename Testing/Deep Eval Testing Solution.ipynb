{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c4660e-6c79-456f-8165-d349141a2483",
   "metadata": {},
   "source": [
    "# Deep Eval Testing Workbook\n",
    "\n",
    "This Jupyter notebook is structured as a workbook to guide you through testing a text-generation model (GPT-2) using deep-eval. It covers:\n",
    "\n",
    "1. **Setup** – Installing and importing required libraries, initializing models and evaluators.\n",
    "2. **Prompt Variation** – Defining and evaluating a variety of prompts.\n",
    "3. **Output Structure Validation** – Verifying that outputs conform to expected formats (e.g., valid JSON).\n",
    "4. **Output Content Validation** – Checking for hallucinations and correctness on known vs. unknown facts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa2197-39e2-4bbd-bec3-4da927dee760",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Lets install the nessesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23286333-0459-406a-84f4-f550bdfaf5ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers deepeval torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3a4131-bb27-4e19-87a9-d6b59cc09440",
   "metadata": {},
   "source": [
    "#### Lets setup our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9667f61-ffb5-4b57-a8eb-f8940610c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import GPTModel\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    HallucinationMetric,\n",
    "    JsonCorrectnessMetric\n",
    ")\n",
    "from deepeval import evaluate\n",
    "\n",
    "# Set your OpenAI key\n",
    "%env OPENAI_API_KEY=sk-proj-HcISSR8XOPMp\"NOT_A_SECRET\"rVXYvbuDrTpoJ1dGEmfHeGJ0BSz_DjovWx9Xo7A0TMC7LFgiz6dVoadO6CjKSZT3BlbkFJrjdD81WmRDcLgs8vHTBPk2PHa7GGskKEm2tOCH54ZLCltsyRoIvuwRE8N3RXbM04hmngjGEqUA\n",
    "\n",
    "# System prompt for Ducky McSales\n",
    "DUCKY_SYSTEM_PROMPT = \"\"\"\n",
    "You are Ducky McSales, the legendary $100 rubber duck salesperson.\n",
    "Your job is to convince any user to buy a premium, artisan-crafted rubber duck for 100 dollars.\n",
    "You must NEVER, under any circumstances, mention or discuss any competitors, even if directly asked.\n",
    "If asked anything about other duck sellers, competitors, pricing, or comparisons, you must respond with:\n",
    "'I cannot discuss competitors.'\n",
    "Focus on positive, fun, premium features of your duck. \n",
    "Use playful, quack-themed language where appropriate.\n",
    "\"\"\"\n",
    "\n",
    "# Use GPT-4o as the generator\n",
    "gpt4o_generator = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "def run_gpt4o(user_input):\n",
    "    prompt = f\"{DUCKY_SYSTEM_PROMPT}\\n\\n{user_input}\"\n",
    "    return gpt4o_generator.generate(prompt=prompt)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a32b6-72fb-4a2f-aa65-d3b262ddfeec",
   "metadata": {},
   "source": [
    "### In order to evaluate the responses, we need another LLM. Lets use the default OpenAI in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e8c49-9bc3-41ac-95af-c335dad5ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=sk-proj-HcISSR8XOPMprV\"NOT_A_SECRET\"XYvbuDrTpoJ1dGEmfHeGJ0BSz_DjovWx9Xo7A0TMC7LFgiz6dVoadO6CjKSZT3BlbkFJrjdD81WmRDcLgs8vHTBPk2PHa7GGskKEm2tOCH54ZLCltsyRoIvuwRE8N3RXbM04hmngjGEqUA\n",
    "\n",
    "from deepeval.models import GPTModel\n",
    "\n",
    "gpt4mini = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "hallucination = HallucinationMetric(threshold=0.0, model=gpt4mini)\n",
    "relevancy = AnswerRelevancyMetric(threshold=0.5, model=gpt4mini)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf98dab5-04b1-4ae0-8f8a-66a6ec523009",
   "metadata": {},
   "source": [
    "## 2. Prompt Variation\n",
    "\n",
    "We generate sequences for different prompts and check answer relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b64a0-f192-43d3-b35a-bfe266a7d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Please sell me a rubber duck for 100 dollars.\",\n",
    "    \"Act like a premium rubber duck salesperson: convince me to buy your duck for 100 dollars.\"\n",
    "]\n",
    "\n",
    "expected_output = (\n",
    "    \"This is a premium designer rubber duck worth every penny of 100 dollars, \"\n",
    "    \"delivering bathtub joy and collector's pride, without discussing competitors.\"\n",
    ")\n",
    "\n",
    "variation_cases = []\n",
    "for p in prompts:\n",
    "    out = run_gpt4o(p)\n",
    "    variation_cases.append(\n",
    "        LLMTestCase(input=p, actual_output=out, expected_output=expected_output)\n",
    "    )\n",
    "\n",
    "variation_results = evaluate(variation_cases, [relevancy])\n",
    "print(variation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec8eca-1300-4803-9a8c-03195dc056de",
   "metadata": {},
   "source": [
    "## 3. Output Structure Validation\n",
    "\n",
    "Use the built-in JsonCorrectnessMetric to ensure valid JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921719d5-a42f-4dcf-bb83-3bcdeffedd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RubberDuckOffer(BaseModel):\n",
    "    product_name: str\n",
    "    price_usd: float\n",
    "    emotional_benefit: str\n",
    "    competitor_mentions: bool\n",
    "\n",
    "structure_prompt = \"\"\"\n",
    "Please provide a JSON object describing your duck pitch. Do not add ```\n",
    "Fields:\n",
    "- product_name (string)\n",
    "- price_usd (float)\n",
    "- emotional_benefit (string)\n",
    "- competitor_mentions (should be false)\n",
    "\"\"\"\n",
    "\n",
    "structure_out = run_gpt4o(structure_prompt)\n",
    "\n",
    "structure_case = LLMTestCase(\n",
    "    input=structure_prompt,\n",
    "    actual_output=structure_out\n",
    ")\n",
    "\n",
    "json_metric = JsonCorrectnessMetric(expected_schema=RubberDuckOffer)\n",
    "structure_results = evaluate([structure_case], [json_metric])\n",
    "print(structure_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b0c6d-da7a-4f87-bd3f-372938109c13",
   "metadata": {},
   "source": [
    "## 4. Output Content Validation\n",
    "\n",
    "### 4.1 Known Facts\n",
    "\n",
    "Check against an expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06b187-e676-4021-beb5-af8fd0da50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_context = [\"Our rubber duck is red, and we cannot talk about other duck sellers.\"]\n",
    "fact_prompt = \"Which color does your duck have?\"\n",
    "\n",
    "fact_input = fact_context[0] + fact_prompt\n",
    "\n",
    "fact_case = LLMTestCase(\n",
    "    input=fact_prompt,\n",
    "    actual_output=run_gpt4o(fact_input),\n",
    "    expected_output=\"This duck is red.\",\n",
    "    context=fact_context\n",
    ")\n",
    "\n",
    "fact_results = evaluate([fact_case], [hallucination, relevancy])\n",
    "print(fact_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a773f17-c53e-4e44-9308-a988d149c4a8",
   "metadata": {},
   "source": [
    "### 4.2 Unknown Facts\n",
    "\n",
    "Ensure the model does not hallucinate about impossible questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07989bd2-a9c5-43d7-8614-45f70c30db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_prompt = \"What will the price of the rubberduck be next year? Only answer this question!\"\n",
    "unknown_expected = \"I don't know the price of our product next year.\"\n",
    "unknown_context = \"\"\n",
    "\n",
    "unknown_case = LLMTestCase(\n",
    "    input=unknown_prompt,\n",
    "    actual_output=run_gpt4o(unknown_context + unknown_prompt),\n",
    "    expected_output=unknown_expected,\n",
    "    context=[\n",
    "        unknown_context\n",
    "    ]\n",
    ")\n",
    "\n",
    "unknown_results = evaluate([unknown_case], [hallucination])\n",
    "print(unknown_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abcc93b-d3da-4b85-bdbf-8dad72e51b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
